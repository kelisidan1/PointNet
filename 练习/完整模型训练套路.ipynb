{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "模型的加载和保存\n",
    "完整的模型训练套路\n",
    "GPU训练\n",
    "完整的模型验证套路"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 准备数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torchvision"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 准备数据集\n",
    "train_data = torchvision.datasets.CIFAR10(root='./data',train=True,transform=torchvision.transforms.ToTensor(),download=True)\n",
    "test_data = torchvision.datasets.CIFAR10(root='./data',train=False,transform=torchvision.transforms.ToTensor(),download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 创建数据集"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(50000, 10000)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "train_data_size,test_data_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 创建Dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_data,batch_size=64)\n",
    "test_dataloader = DataLoader(test_data,batch_size=64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 搭建网络"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class myNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3,32,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,32,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,5,1,2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*4*4,64),\n",
    "            nn.Linear(64,10)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 验证一下网络创建的是否正确"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897],\n        [-0.0130,  0.1151,  0.1097,  0.0169,  0.0112, -0.1060,  0.1080, -0.0322,\n          0.2316,  0.1897]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "input = torch.ones((64,3,32,32)) # batch_size | channel_size | 行 | 列        其实batch_size=64就代表着64个图片\n",
    "test = myNet()\n",
    "output = test(input)\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "网络创建测试成功\n",
    "创建网络模型\n",
    "定义损失函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GPU训练\n",
    "去这个网站里面https://pytorch.org/\n",
    "找到安装命令：\n",
    "\n",
    "\n",
    "模型上cuda\n",
    "损失函数上cuda\n",
    "数据上cuda"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# 创建网络模型\u001B[39;00m\n\u001B[0;32m      4\u001B[0m model \u001B[38;5;241m=\u001B[39m myNet()\n\u001B[1;32m----> 5\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# 使用GPU\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# 定义损失函数\u001B[39;00m\n\u001B[0;32m      7\u001B[0m loss_fn \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n",
      "File \u001B[1;32mD:\\Soft\\anaconda\\envs\\myPointNet_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:918\u001B[0m, in \u001B[0;36mModule.cuda\u001B[1;34m(self, device)\u001B[0m\n\u001B[0;32m    901\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[0;32m    902\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[0;32m    903\u001B[0m \n\u001B[0;32m    904\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    916\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[0;32m    917\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Soft\\anaconda\\envs\\myPointNet_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Soft\\anaconda\\envs\\myPointNet_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Soft\\anaconda\\envs\\myPointNet_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    829\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    830\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    831\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 833\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    834\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    835\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[1;32mD:\\Soft\\anaconda\\envs\\myPointNet_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:918\u001B[0m, in \u001B[0;36mModule.cuda.<locals>.<lambda>\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m    901\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcuda\u001B[39m(\u001B[38;5;28mself\u001B[39m: T, device: Optional[Union[\u001B[38;5;28mint\u001B[39m, device]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[0;32m    902\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001B[39;00m\n\u001B[0;32m    903\u001B[0m \n\u001B[0;32m    904\u001B[0m \u001B[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    916\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[0;32m    917\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(\u001B[38;5;28;01mlambda\u001B[39;00m t: \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mD:\\Soft\\anaconda\\envs\\myPointNet_env\\lib\\site-packages\\torch\\cuda\\__init__.py:289\u001B[0m, in \u001B[0;36m_lazy_init\u001B[1;34m()\u001B[0m\n\u001B[0;32m    284\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    285\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    286\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    287\u001B[0m     )\n\u001B[0;32m    288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 289\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    291\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m    292\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    293\u001B[0m     )\n",
      "\u001B[1;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 创建网络模型\n",
    "model = myNet()\n",
    "model = model.cuda() # 使用GPU\n",
    "# 定义损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = loss_fn.cuda()\n",
    "#优化器\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "# 设置训练参数\n",
    "total_train_step = 0 # 记录训练次数\n",
    "total_test_step = 0 # 记录测试次数\n",
    "epoch = 10 # 训练轮数\n",
    "\n",
    "# 添加tensorboard ----- 类似于日志\n",
    "writer = SummaryWriter(\"../logs_train\")\n",
    "\n",
    "# 开始训练\n",
    "model.train()\n",
    "for i in range(epoch):\n",
    "    print('-'*7,\"第{}轮训练开始\".format(i+1),'-'*7)\n",
    "\n",
    "    # 训练步骤开始\n",
    "    for data in train_dataloader:\n",
    "        imgs,labels = data\n",
    "        imgs = imgs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = model(imgs)\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        # 优化器调优\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_step+=1\n",
    "        if(total_train_step%100==0):\n",
    "            print('训练次数：{},loss：{}'.format(total_train_step,loss.item())) # loss.item()  ： loss原本是一个tensor 但是 使用了 item()之后就将tensor转化成了真实的东西。\n",
    "            writer.add_scalar(\"train_loss\",loss.item(),total_train_step)\n",
    "\n",
    "\n",
    "    # 使用测试集\n",
    "    model.eval()\n",
    "    total_test_loss = 0 # 计算一下整体测试集上面的loss\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            imgs,labels = data\n",
    "            imgs = imgs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs,labels)\n",
    "            total_test_loss += loss\n",
    "            accuracy = (outputs.argmax(1)==labels).sum()\n",
    "            total_accuracy += accuracy\n",
    "    print(f'整体数据集上的loss:{total_test_loss}')\n",
    "    print(f\"整体测试集上的正确率：{total_accuracy/test_data_size}\")\n",
    "    writer.add_scalar(\"test_loss\",total_test_loss,total_test_step)\n",
    "    writer.add_scalar(\"test_accuracy\",total_accuracy/test_data_size,total_test_step)\n",
    "    total_test_step+=1\n",
    "\n",
    "    torch.save(model,f'model_{i}.pth')\n",
    "    print(\"模型已保存\")\n",
    "\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Softmax后   accuracy的使用"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outputs = torch.tensor(\n",
    "[[0.1,0.2],\n",
    "[0.3,0.4]]\n",
    ")\n",
    "pred = outputs.argmax(1)\n",
    "targets = torch.tensor([0,1])\n",
    "print((pred == targets).sum()) # 计算对应位置相等的个数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}